# Errordon Docker Compose Template v0.3.0
# Copy to your server and adjust values
#
# Usage:
#   cp deploy/docker-compose.yml .
#   cp deploy/.env.example .env.production
#   docker compose up -d
#
# NSFW-Protect AI (optional):
#   Uncomment the 'ollama' service below for AI content moderation
#   Or install Ollama on the host: curl -fsSL https://ollama.com/install.sh | sh

version: '3.8'

services:
  db:
    image: postgres:15-alpine
    restart: always
    shm_size: 256mb
    networks:
      - internal_network
    healthcheck:
      test: ['CMD', 'pg_isready', '-U', 'postgres']
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust

  redis:
    image: redis:7-alpine
    restart: always
    networks:
      - internal_network
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
    volumes:
      - redis_data:/data

  web:
    # Option 1: Use official Mastodon image (without Errordon features)
    # image: ghcr.io/mastodon/mastodon:v4.3.0
    # Option 2: Build Errordon from source (recommended)
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    env_file: .env.production
    command: bundle exec puma -C config/puma.rb
    networks:
      - external_network
      - internal_network
    healthcheck:
      test: ['CMD-SHELL', 'wget -q --spider --proxy=off localhost:3000/health || exit 1']
    ports:
      - '127.0.0.1:3000:3000'
    depends_on:
      - db
      - redis
    volumes:
      - mastodon_public:/mastodon/public/system

  streaming:
    # image: ghcr.io/mastodon/mastodon-streaming:v4.3.0
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    env_file: .env.production
    command: node ./streaming/index.js
    networks:
      - external_network
      - internal_network
    healthcheck:
      test: ['CMD-SHELL', 'wget -q --spider --proxy=off localhost:4000/api/v1/streaming/health || exit 1']
    ports:
      - '127.0.0.1:4000:4000'
    depends_on:
      - db
      - redis

  sidekiq:
    # image: ghcr.io/mastodon/mastodon:v4.3.0
    build:
      context: .
      dockerfile: Dockerfile
    restart: always
    env_file: .env.production
    command: bundle exec sidekiq
    networks:
      - external_network
      - internal_network
    depends_on:
      - db
      - redis
    volumes:
      - mastodon_public:/mastodon/public/system
    healthcheck:
      test: ['CMD-SHELL', "ps aux | grep '[s]idekiq 6' || false"]

  # ============================================================================
  # NSFW-PROTECT AI (Optional)
  # ============================================================================
  # Uncomment to run Ollama in Docker (alternative to host installation)
  # Note: Requires GPU support for optimal performance
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   restart: always
  #   networks:
  #     - internal_network
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   # Uncomment for NVIDIA GPU support:
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]
  #   # After starting, pull models:
  #   #   docker compose exec ollama ollama pull llava
  #   #   docker compose exec ollama ollama pull llama3

networks:
  external_network:
  internal_network:
    internal: true

volumes:
  postgres_data:
  redis_data:
  mastodon_public:
  # Uncomment if using Ollama in Docker:
  # ollama_data:
