# Errordon Docker Compose v1.2.0
# Quick Install: curl -sSL https://raw.githubusercontent.com/error-wtf/errordon/master/deploy/interactive-install.sh | bash

services:
  db:
    image: postgres:14-alpine
    restart: always
    shm_size: 256mb
    networks:
      - internal_network
    healthcheck:
      test: ['CMD', 'pg_isready', '-U', 'postgres']
    volumes:
      - ./postgres14:/var/lib/postgresql/data
    environment:
      POSTGRES_HOST_AUTH_METHOD: trust

  redis:
    image: redis:7-alpine
    restart: always
    networks:
      - internal_network
    healthcheck:
      test: ['CMD', 'redis-cli', 'ping']
    volumes:
      - ./redis:/data

  # ============================================================================
  # OPENSEARCH (Full-Text Search)
  # ============================================================================
  es:
    image: opensearchproject/opensearch:2.11.0
    restart: always
    environment:
      - "OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m"
      - "bootstrap.memory_lock=true"
      - "cluster.name=es-mastodon"
      - "discovery.type=single-node"
      - "DISABLE_SECURITY_PLUGIN=true"
      - "DISABLE_INSTALL_DEMO_CONFIG=true"
    networks:
      - external_network
      - internal_network
    healthcheck:
      test: ["CMD-SHELL", "curl --silent --fail localhost:9200/_cluster/health || exit 1"]
    volumes:
      - ./elasticsearch:/usr/share/opensearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - '127.0.0.1:9200:9200'

  web:
    build:
      context: ..
      dockerfile: Dockerfile
    restart: always
    env_file: .env.production
    command: bundle exec puma -C config/puma.rb
    networks:
      - external_network
      - internal_network
    healthcheck:
      test: ['CMD-SHELL', 'curl -s --noproxy localhost localhost:3000/health | grep -q OK || exit 1']
    ports:
      - '127.0.0.1:3000:3000'
    depends_on:
      - db
      - redis
      - es
    volumes:
      - ./public/system:/opt/mastodon/public/system

  streaming:
    build:
      context: ..
      dockerfile: streaming/Dockerfile
    restart: always
    env_file: .env.production
    command: node ./streaming/index.js
    networks:
      - external_network
      - internal_network
    healthcheck:
      test: ['CMD-SHELL', 'curl -s --noproxy localhost localhost:4000/api/v1/streaming/health | grep -q OK || exit 1']
    ports:
      - '127.0.0.1:4000:4000'
    depends_on:
      - db
      - redis

  sidekiq:
    build:
      context: ..
      dockerfile: Dockerfile
    restart: always
    env_file: .env.production
    command: bundle exec sidekiq
    networks:
      - external_network
      - internal_network
    depends_on:
      - db
      - redis
      - es
    volumes:
      - ./public/system:/opt/mastodon/public/system
    healthcheck:
      test: ['CMD-SHELL', "ps aux | grep '[s]idekiq' || false"]

  # ============================================================================
  # NSFW-PROTECT AI (Optional)
  # ============================================================================
  # Uncomment to run Ollama in Docker (alternative to host installation)
  # Note: Requires GPU support for optimal performance
  #
  # ollama:
  #   image: ollama/ollama:latest
  #   restart: always
  #   networks:
  #     - internal_network
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   # Uncomment for NVIDIA GPU support:
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities: [gpu]
  #   # After starting, pull models:
  #   #   docker compose exec ollama ollama pull llava
  #   #   docker compose exec ollama ollama pull llama3

networks:
  external_network:
  internal_network:
    internal: true

# Note: Using bind mounts (./folder) instead of named volumes for easier backup/migration
